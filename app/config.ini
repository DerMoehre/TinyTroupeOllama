
[OpenAI]
api_type = ollama

model = qwen3:8b

# [Ollama] section:
# These settings are specific to connecting to your Ollama service.
[Ollama]
# for local testing, you need to set the base_url to localhost
base_url = http://ollama:11434/api/chat

# model: This references the OLLAMA_MODEL environment variable from your .env file.
# TinyTroupe will use the model specified there.
model = qwen3:8b

# temperature: Controls the randomness of the model's output. Lower means more deterministic.
temperature = 0.5

# top_p: Controls token selection. A lower value means only the most probable tokens are considered.
top_p = 0.95

# timeout: How long TinyTroupe waits for a response from Ollama before giving up (in seconds).
timeout = 120